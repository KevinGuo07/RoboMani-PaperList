# A Paperlist for RoboMani-Learning üöÄü§ñ

[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)]()
<!--[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)-->

---

## üìå Basic Info

This repository collects the latest and influential papers and resources related to **robotic manipulation**. The focus is on:

- [Generalist Manipulation Models & Methods](#-generalist-manipulation-models-and-methods)
- [Reinforcement Learning (RL) on Robotics Manipulation](#-reinforcement-learning-rl-on-robotics-manipulation)
- [World Model in Robot Manipulation](#-world-model-in-robot-manipulation)
- [Skill Learning in Robotics](#-skill-learning-in-robotics) 
- [Data & Benchmarks](#-data-and-benchmarks) 
- [Hardware Projects on Robotics](#-hardware-projects-on-robotics) 
- [Interdisciplinary](#-interdisciplinary)

The following publications are ordered by time, with the most recent at the top.

Papers with **open-sourced implementations or code** are marked with a ‚òÄÔ∏è  
Papers with **real-world performance** reproduced by us are marked with a ‚úÖ  
Works claimed with **code coming soon** are marked with a üßê  

---

## üìö Paper List

### üß† Generalist Manipulation Models and Methods
- **WALL-OSS**: Igniting VLMs toward the Embodied Space [[paper](https://arxiv.org/pdf/2509.11766)] [[project](https://github.com/X-Square-Robot/wall-x)] ‚òÄÔ∏è  
- **TA-VLA**: Elucidating the Design Space of Torque-aware Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2509.07962)] [[project](https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/)] üßê
- **F1**: A VISION-LANGUAGE-ACTION MODEL BRIDGING UNDERSTANDING AND GENERATION TO ACTIONS [[paper](https://arxiv.org/pdf/2509.06951v2)] [[project](https://aopolin-lv.github.io/F1-VLA/)] ‚òÄÔ∏è  
- **RaC**: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction [[paper](https://arxiv.org/pdf/2509.07953)] [[project](https://rac-scaling-robot.github.io/)] üßê
- **FlowVLA**: Thinking in Motion with a Visual Chain of Thought [[paper](https://arxiv.org/pdf/2508.18269)]
- **CAST**: Counterfactual Labels Improve Instruction Following inVision-Language-Action Models [[paper](https://arxiv.org/pdf/2508.13446)] [[project](https://cast-vla.github.io/)] ‚òÄÔ∏è
- **Grounding Actions in Camera Space**: Observation-CentricVision-Language-Action Policy [[paper](https://arxiv.org/pdf/2508.13103)]
- **GeoVLA**: Empowering 3D Representations in Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2508.09071)] [[project](https://linsun449.github.io/GeoVLA//)] üßê
- **GraphCoT-VLA**: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions [[paper](https://arxiv.org/pdf/2508.07650)]
- **InstructVLA**: Vision-Language-Action Instruction Tuning: From Understanding to Manipulation [[paper](https://arxiv.org/pdf/2507.17520)] [[project](https://yangs03.github.io/InstructVLA_Home/)] ‚òÄÔ∏è
- **Being-H0**: Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos [[paper](https://arxiv.org/pdf/2507.15597)] [[project](https://beingbeyond.github.io/Being-H0/)] ‚òÄÔ∏è
- **EgoVLA**: Learning Vision-Language-Action Models from Egocentric Human Videos [[paper](https://arxiv.org/pdf/2507.12440)] [[project](https://rchalyang.github.io/EgoVLA/)]
- **ThinkAct**: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning [[paper](https://arxiv.org/pdf/2507.16815)] [[project](https://jasper0314-huang.github.io/thinkact-vla/)] üßê
- **GR-3** Technical Report [[paper](https://arxiv.org/pdf/2507.15493)] [[project](https://seed.bytedance.com/en/GR3)]
- **VITA**: VISION-TO-ACTION FLOW MATCHING POLICY [[paper](https://arxiv.org/pdf/2507.13231)] [[project](https://ucd-dare.github.io/VITA/)] üßê
- **TACTILE-VLA**: UNLOCKING VISION-LANGUAGE ACTION MODEL‚ÄôS PHYSICAL KNOWLEDGE FOR TACTILE GENERALIZATION [[paper](https://arxiv.org/pdf/2507.09160)]
- **VOTE**: Vision-Language-Action Optimization with Trajectory Ensemble Voting [[paper](https://arxiv.org/pdf/2507.05116)] [[project](https://github.com/LukeLIN-web/VOTE)] ‚òÄÔ∏è
- **Evo-0**: Vision-Language-Action Model with Implicit Spatial Understanding [[paper](https://www.arxiv.org/pdf/2507.00416)] [[project](https://mint-sjtu.github.io/Evo-VLA.io/)] üßê
- **TRIVLA**: A TRIPLE-SYSTEM-BASED UNIFIED VISION-LANGUAGE-ACTION MODEL FOR GENERAL ROBOT CONTROL [[paper](https://arxiv.org/pdf/2507.01424)] [[project](https://zhenyangliu.github.io/TriVLA/)] üßê
- **WorldVLA**: Towards Autoregressive Action World Model [[paper](https://arxiv.org/pdf/2506.21539)] [[project](https://github.com/alibaba-damo-academy/WorldVLA)] ‚òÄÔ∏è
- Grounding Language Models with Semantic Digital Twins for Robotic Planning [[paper](https://arxiv.org/pdf/2506.16493)]
- **Prompting with the Future**:Open-WorldModel PredictiveControlwithInteractiveDigitalTwins [[paper](https://arxiv.org/pdf/2506.13761)] [[project](https://prompting-with-the-future.github.io/)] ‚òÄÔ∏è
- **RationalVLA**: A Rational Vision-Language-Action Model with Dual System [[paper](https://arxiv.org/pdf/2506.10826)] [[project](https://irpn-eai.github.io/RationalVLA/)] üßê
- **Chain-of-Action**: Trajectory Autoregressive Modeling for Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.09990)] [[project](https://chain-of-action.github.io/)] ‚òÄÔ∏è
- **BitVLA**: 1-bit Vision-Language-Action Models for Robotics Manipulation [[paper](https://arxiv.org/pdf/2506.07530)]
- **PDFactor**: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.pdf)]
- **FlowRAM**: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.pdf)]
- Real-Time Execution of Action Chunking Flow Policies [[paper](https://www.physicalintelligence.company/download/real_time_chunking.pdf)] ‚òÄÔ∏è
- **SmolVLA**: A Vision-Language-Action Model for Affordable and Efficient Robotics [[paper](https://arxiv.org/pdf/2506.01844)] [[project](https://huggingface.co/lerobot/smolvla_base)] ‚òÄÔ∏è ‚úÖ
- Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better [[paper](https://arxiv.org/pdf/2505.23705)] [[project](https://www.physicalintelligence.company/research/knowledge_insulation)]
- **ChatVLA-2**: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge [[paper](https://arxiv.org/pdf/2505.21906)] [[project](https://chatvla-2.github.io/)] ‚òÄÔ∏è
- **ForceVLA**: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation [[paper](https://arxiv.org/pdf/2505.22159)] [[project](https://sites.google.com/view/forcevla2025/)]
- **DexUMI**: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation [[paper](https://arxiv.org/pdf/2505.21864)] [[project](https://dex-umi.github.io/)] ‚òÄÔ∏è
- **Hume**: Introducing System-2 Thinking in Visual-Language-Action Model [[paper](https://arxiv.org/pdf/2505.21432)]
- **FLARE**: Robot Learning with Implicit World Modeling [[paper](https://arxiv.org/pdf/2505.15659)]
- **InSpire**: Vision-Language-Action Models with Intrinsic Spatial Reasoning [[paper](https://arxiv.org/pdf/2505.13888)] [[project](https://koorye.github.io/proj/Inspire/)] ‚òÄÔ∏è
- **DreamGen**: Unlocking Generalization in Robot Learning through Neural Trajectories  [[paper](https://arxiv.org/pdf/2505.12705)] [[project](https://research.nvidia.com/labs/gear/dreamgen/)] ‚òÄÔ∏è
- **GLOVER++**: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation [[paper](https://arxiv.org/pdf/2505.11865)] [[project](https://teleema.github.io/projects/GLOVER++/)] ‚òÄÔ∏è
- **UniVLA**: Learning to Act Anywhere with Task-centric Latent Action [[paper](https://arxiv.org/pdf/2505.06111)] [[project](https://github.com/OpenDriveLab/UniVLA)] ‚òÄÔ∏è
- **NORA**: A SMALL OPEN-SOURCED GENERALIST VISION LANGUAGE ACTION MODEL FOR EMBODIED TASKS [[paper](https://arxiv.org/pdf/2504.19854)] [[project](https://declare-lab.github.io/nora)] ‚òÄÔ∏è ‚úÖ
- **Gemini Robotics**: Bringing AI into the Physical World [[paper](https://arxiv.org/abs/2503.20020)] [[project](https://deepmind.google/models/gemini-robotics/)] ‚òÄÔ∏è
- **GR00T N1**: An Open Foundation Model for Generalist Humanoid Robots [[paper](https://arxiv.org/pdf/2503.14734)] [[project](https://developer.nvidia.com/isaac/gr00t)] ‚òÄÔ∏è ‚úÖ
- **œÄ0.5**: a VLA with Open-World Generalization [[paper](https://www.physicalintelligence.company/download/pi05.pdf)] [[project](https://www.physicalintelligence.company/blog/pi05)] ‚òÄÔ∏è
- **PointVLA**: Injecting the 3D World into Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2503.07511)] [[project](https://pointvla.github.io/)]
- **GraspVLA**: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data [[paper](https://arxiv.org/pdf/2505.03233)] [[project](https://pku-epic.github.io/GraspVLA-web/)] ‚òÄÔ∏è
- **Reactive Diffusion Policy**: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.02881)] [[project](https://reactive-diffusion-policy.github.io/)] ‚òÄÔ∏è
- **Unified Video Action Model** [[paper](https://arxiv.org/pdf/2503.00200)] [[project](https://unified-video-action-model.github.io/)] ‚òÄÔ∏è
- **OpenVLA-OFT**: Fine-Tuning Vision-Language-Action Models:Optimizing Speed and Success [[paper](https://arxiv.org/pdf/2502.19645)] [[project](https://openvla-oft.github.io)] ‚òÄÔ∏è ‚úÖ
- **Helix**:Helix: A Vision-Language-Action Model for Generalist Humanoid Control [[project](https://www.figure.ai/news/helix)]
- **You Only Teach Once**: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations [[paper](https://arxiv.org/pdf/2501.14208)] [[project](https://hnuzhy.github.io/projects/YOTO/)] ‚òÄÔ∏è
- **TraceVLA**: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [[paper](https://arxiv.org/pdf/2412.10345)] [[project](https://tracevla.github.io/)] ‚òÄÔ∏è
- **CogACT**: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://arxiv.org/pdf/2411.19650)] [[project](https://cogact.github.io/)] ‚òÄÔ∏è ‚úÖ
- **GRAPE**: Generalizing Robot Policy via Preference Alignment [[paper](https://arxiv.org/pdf/2411.19309)] [[project](https://grape-vla.github.io/)] ‚òÄÔ∏è
- **iDP3**: Generalizable Humanoid Manipulation with 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)] [[project](https://humanoid-manipulation.github.io/)] ‚òÄÔ∏è
- **œÄ0**: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://arxiv.org/pdf/2410.24164)] [[project](https://www.physicalintelligence.company/blog/pi0)] ‚òÄÔ∏è ‚úÖ
- **TinyVLA**: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)] [[project](https://arxiv.org/pdf/2409.12514)] ‚òÄÔ∏è
- **ReKep**: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.01652)] [[project](https://rekep-robot.github.io/)] ‚òÄÔ∏è ‚úÖ
- **OpenVLA**: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)] [[project](https://openvla.github.io/)] ‚òÄÔ∏è ‚úÖ
- **3D Diffusion Policy**: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)] [[project](https://3d-diffusion-policy.github.io/)] ‚òÄÔ∏è ‚úÖ

### üîÅ Reinforcement Learning (RL) on Robotics Manipulation
- Residual Off-Policy RL for Finetuning Behavior Cloning Policies [[paper](https://arxiv.org/pdf/2509.19301)] [[project](https://residual-offpolicy-rl.github.io//)] üßê
- **SOE**: Sample-Efficient Robot Policy Self-Improvementvia On-Manifold Exploration [[paper](https://arxiv.org/pdf/2509.19292)] [[project](https://ericjin2002.github.io/SOE/)] üßê
- **RLinf**: Reinforcement Learning Infrastructure for Agentic AI [[paper](https://rlinf.readthedocs.io/en/latest/)] [[project](https://github.com/RLinf/RLinf)] ‚òÄÔ∏è
- Compute-Optimal Scaling for Value-Based Deep RL [[paper](https://arxiv.org/pdf/2508.14881)]
- **Embodied-R1**: Reinforced Embodied Reasoningfor General Robotic Manipulation [[paper](https://arxiv.org/pdf/2508.13998)] [[project](https://embodied-r1.github.io/)] ‚òÄÔ∏è
- **CO-RFT**: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning [[paper](https://arxiv.org/pdf/2508.02219)]
- **ROVER**: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks [[paper](https://arxiv.org/pdf/2508.01943)] [[project](https://rover-vlm.github.io/)] ‚òÄÔ∏è
- Reinforcement Learning for Flow-Matching Policies [[paper](https://arxiv.org/pdf/2507.15073v1)]
- **FOUNDER**: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making [[paper](https://arxiv.org/pdf/2507.12496)] [[project](https://sites.google.com/view/founder-rl)]
- **EXPO**: Stable Reinforcement Learning with Expressive Policies [[paper](https://arxiv.org/pdf/2507.07986)] 
- Asynchronous multi-agent deep reinforcement learning under partial observability [[paper](https://journals.sagepub.com/doi/full/10.1177/02783649241306124)]
- Reinforcement Learning with Action Chunking [[paper](https://www.alphaxiv.org/abs/2507.07969v1)] [[project](https://github.com/ColinQiyangLi/qc)] ‚òÄÔ∏è ‚úÖ
- **SimLauncher**: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training [[paper](https://arxiv.org/pdf/2507.04452)]
- **RLRC**: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2506.17639)] [[project](https://rlrc-vla.github.io/)] üßê
- Steering Your Diffusion Policy with Latent Space Reinforcement Learning [[paper](https://arxiv.org/pdf/2506.15799)] [[project](https://diffusion-steering.github.io/)] ‚òÄÔ∏è
- **GMT**:General Motion Tracking for Humanoid Whole-Body Control [[paper](https://arxiv.org/pdf/2506.14770)] [[project](https://gmt-humanoid.github.io/)] ‚òÄÔ∏è
- **Eye, Robot**: Learning to Look to Act with a BC-RL Perception-Action Loop [[paper](https://arxiv.org/pdf/2506.10968)] [[project](https://www.eyerobot.net/)] üßê
- Reinforcement Learning via Implicit Imitation Guidance [[paper](https://arxiv.org/pdf/2506.07505)]
- Robotic Policy Learning via Human-assisted Action Preference Optimization [[paper](https://arxiv.org/pdf/2506.07127)] [[project](https://gewu-lab.github.io/hapo_human_assisted_preference_optimization/)] üßê
- Self-Adapting Improvement Loops for Robotic Learning [[paper](https://arxiv.org/pdf/2506.07505)] [[project](https://diffusion-supervision.github.io/sail/)] üßê
- **Robot-R1**: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics [[paper](https://arxiv.org/pdf/2506.00070v1)] 
- Self-Challenging Language Model Agents [[paper](https://arxiv.org/pdf/2506.01716)] 
- Diffusion Guidance Is a Controllable Policy Improvement Operator [[paper](https://arxiv.org/pdf/2505.23458)] [[project](https://github.com/kvfrans/cfgrl)] ‚òÄÔ∏è
- **Beyond Markovian**: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [[paper](https://arxiv.org/pdf/2505.20561v1)] [[project](https://github.com/shenao-zhang/BARL)] ‚òÄÔ∏è
- What Can RL Bring to VLA Generalization? An Empirical Study [[paper](https://arxiv.org/pdf/2505.19789)] [[project](https://rlvla.github.io/)] ‚òÄÔ∏è
- Learning to Reason without External Rewards [[paper](https://arxiv.org/pdf/2505.19590)]
- **GenPO**: Generative Diffusion Models Meet On-Policy Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.18763)]
- **VLA-RL**: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.18719)][[code](https://github.com/GuanxingLu/vlarl)] ‚òÄÔ∏è ‚úÖ
- **SimpleVLA-RL**: Online RL with Simple Reward Enables Training VLA Models with Only One Trajectory [[paper](https://www.alphaxiv.org/abs/2509.09674)][[code](https://github.com/PRIME-RL/SimpleVLA-RL)] ‚òÄÔ∏è ‚úÖ
- **TeViR**: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.19769)] 
- **Genie Centurion**: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance [[paper](https://arxiv.org/pdf/2505.18793)] [[project](https://genie-centurion.github.io/)] ‚òÄÔ∏è
- **RIPT-VLA**: Interactive Post-Training for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2505.17016)] [[project](https://ariostgx.github.io/ript_vla/)] ‚òÄÔ∏è 
- **ManipLVM-R1**: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models [[paper](https://arxiv.org/pdf/2505.16517)]
- Deep reinforcement learning for robotic manipulation [[technique report](https://patentimages.storage.googleapis.com/7f/04/95/2437c0dc1b5ab6/US20250153352A1.pdf)]
- **DORA**:Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation [[paper](https://arxiv.org/pdf/2505.14819)] [[project](https://sites.google.com/view/dora-manip)] üßê
- What Matters for Batch Online Reinforcement Learning in Robotics? [[paper](https://arxiv.org/pdf/2505.08078)] 
- **ReinboT**: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.07395)] 
- **IN‚ÄìRIL**: Interleaved Reinforement and Imitation Learning for Policy Fine-tuning [[paper](https://arxiv.org/pdf/2505.10442)] [[project](https://github.com/ucd-dare/IN-RIL)] üßê
- **MoRE**: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models[[paper](https://arxiv.org/pdf/2503.08007)]
- **ConRFT**: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[paper](https://arxiv.org/pdf/2502.05450)] [[project](https://cccedric.github.io/conrft/)] ‚òÄÔ∏è
- **Rethinking Latent Redundancy in Behavior Cloning**: An Information Bottleneck Approach for Robot Manipulation [[paper](https://arxiv.org/pdf/2502.02853)] [[project](https://baishuanghao.github.io/BC-IB.github.io/)] ‚òÄÔ∏è
- Flow Q-Learning [[paper](https://arxiv.org/pdf/2502.02538)] [[project](https://seohong.me/projects/fql/)] ‚òÄÔ∏è
- Improving Vision-Language-Action Model with Online Reinforcement Learning [[paper](https://arxiv.org/pdf/2501.16664)] 
- **RLDG**: Robotic Generalist Policy Distillation via Reinforcement Learning [[paper](https://arxiv.org/pdf/2412.09858)] [[project](https://generalist-distillation.github.io/)] ‚òÄÔ∏è
- Vision Language Models are In-Context Value Learners [[paper](https://arxiv.org/pdf/2411.04549)] [[project](https://generative-value-learning.github.io/)] ‚òÄÔ∏è
- **Hil-serl**: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning [[paper](https://arxiv.org/pdf/2410.21845)] [[project](https://hil-serl.github.io/)] ‚òÄÔ∏è ‚úÖ
- **SERL**: A Software Suite for Sample-Efficient Robotic Reinforcement Learning [[paper](https://arxiv.org/pdf/2401.16013)] [[project](https://serl-robot.github.io/)] ‚òÄÔ∏è ‚úÖ

### üåç World Model in Robot Manipulation 
- **World4RL**: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation [[paper](https://arxiv.org/pdf/2509.19080)] [[project](https://world4rl.github.io/)]
- Latent Action Pretraining Through World Modeling [[paper](https://arxiv.org/pdf/2509.18428)]
- **OmniWorld**: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling [[paper](https://arxiv.org/pdf/2509.12201)] [[project](https://yangzhou24.github.io/OmniWorld/)] ‚òÄÔ∏è
- World Modeling with Probabilistic Structure Integration [[paper](https://arxiv.org/pdf/2509.09737)]
- Planning with Reasoning using Vision Language World Model [[paper](https://arxiv.org/pdf/2509.02722)]
- **Learning Primitive Embodied World Models**: Towards Scalable Robotic Learning [[paper](https://arxiv.org/pdf/2508.20840)] [[project](https://qiaosun22.github.io/PrimitiveWorld/)] üßê
- **GWM**: Towards Scalable Gaussian World Models for Robotic Manipulation [[paper](https://arxiv.org/abs/2508.17600)] [[project](https://gaussian-world-model.github.io/)] ‚úÖ
- Latent Policy Steering with Embodiment-Agnostic Pretrained World Models [[paper](https://arxiv.org/pdf/2507.13340)]
- Test-Time Scaling with World Models for Spatial Reasoning [[paper](https://arxiv.org/pdf/2507.12508)] [[project](https://umass-embodied-agi.github.io/MindJourney/)] ‚òÄÔ∏è
- **FOUNDER**: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making [[paper](https://arxiv.org/pdf/2507.12496)] [[project](https://sites.google.com/view/founder-rl)]
- **Martian World Models**: Controllable Video Synthesis with Physically Accurate 3D Reconstructions [[paper](https://arxiv.org/pdf/2507.07978)] [[project](https://marsgenai.github.io/)] ‚òÄÔ∏è
- **EmbodieDreamer**: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling [[paper](https://arxiv.org/pdf/2507.05198)] [[project](https://embodiedreamer.github.io/)] üßê
- **DreamVLA**: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge [[paper](https://arxiv.org/pdf/2507.04447)] [[project](https://zhangwenyao1.github.io/DreamVLA/)] ‚òÄÔ∏è
- A Survey: Learning Embodied Intelligence from Physical Simulators and World Models [[paper](https://arxiv.org/pdf/2507.00917)] [[project](https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey)] 
- **RoboScape**: Physics-informed Embodied World Model [[paper](https://arxiv.org/pdf/2506.23135)] [[project](https://github.com/tsinghua-fib-lab/RoboScape)] üßê
- **ParticleFormer**: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.23126)] [[project](https://suninghuang19.github.io/particleformer_page/)] üßê
- **World4Omni**: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.23919)] [[project](https://world4omni.github.io/)] üßê
- **RoboPearls**: Editable Video Simulation for Robot Manipulation [[paper](https://arxiv.org/pdf/2506.22756)] [[project](https://tangtaogo.github.io/RoboPearls/)] üßê
- Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins [[paper](https://arxiv.org/pdf/2506.13761)] [[project](https://prompting-with-the-future.github.io/)] ‚òÄÔ∏è
- **FLARE**: Robot Learning with Implicit World Modeling [[paper](https://arxiv.org/pdf/2505.15659)] [[project](https://research.nvidia.com/labs/gear/flare/)] üßê
- Occupancy World Model for Robots [[paper](https://arxiv.org/pdf/2505.05512)]
- Learning 3D Persistent Embodied World Models [[paper](https://arxiv.org/pdf/2505.05495)]
- **DYNAMICAL DIFFUSION**: LEARNING TEMPORAL DYNAMICS WITH DIFFUSION MODELS [[paper](https://arxiv.org/pdf/2503.00951)] [[project](https://github.com/thuml/dynamical-diffusion)] ‚òÄÔ∏è
- **iVideoGPT**: Interactive VideoGPTs are Scalable World Models [[paper](https://arxiv.org/pdf/2405.15223)] [[project](https://thuml.github.io/iVideoGPT/)] ‚òÄÔ∏è ‚úÖ

### ü¶æ Skill Learning in Robotics

### üì¶ Data and Benchmarks
- **Shortcut Learning in Generalist Robot Policies**: The Role of Dataset Diversity and Fragmentation [[paper](https://arxiv.org/pdf/2508.06426)]
- **FreeTacMan**: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation [[paper](https://arxiv.org/pdf/2506.01941)] [[project](https://freetacmanblog.github.io/)] ‚òÄÔ∏è
- **Guiding Data Collection**: via Factored Scaling Curves [[paper](https://arxiv.org/pdf/2505.07728)] 
- **DemoGen**: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning [[paper](https://arxiv.org/pdf/2502.16932)] [[project](https://demo-generation.github.io/)] ‚òÄÔ∏è
- Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition  [[paper](https://arxiv.org/pdf/2407.00299)] [[project](https://norweig1an.github.io/HAJL.github.io/)] üßê
- **AutoBio**: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory [[paper](https://arxiv.org/pdf/2505.14030) [[project](https://github.com/autobio-bench/AutoBio)] ‚òÄÔ∏è
- **3DFlowAction**: Learning Cross-Embodiment Manipulation from 3D Flow World Model [[paper](https://arxiv.org/pdf/2506.06199) [[project](https://github.com/Hoyyyaard/3DFlowAction/)] ‚òÄÔ∏è
- A very good survey and report on Simulators [[project](https://simulately.wiki)]
- **EgoDex**: Learning Dexterous Manipulation from Large-Scale Egocentric Video [[paper](https://arxiv.org/pdf/2505.11709)]
- **Open X-Embodiment**: Robotic Learning Datasets and RT-X Model [[paper](https://arxiv.org/pdf/2310.08864)] [[project](https://robotics-transformer-x.github.io/)] ‚úÖ
- **GENMANIP**: LLM-driven Simulation for Generalizable Instruction-Following Manipulation [[paper](https://arxiv.org/pdf/2506.10966)] [[project](https://genmanip.axi404.top/)] ‚úÖ
- **RoboArena**: Distributed Real-World Evaluation of Generalist Robot Policies [[paper](https://robo-arena.github.io/assets/roboarena-B1XSLVwD.pdf)] [[project](https://robo-arena.github.io/)]
- **RoboCerebra**:ALarge-scaleBenchmarkfor Long-horizonRoboticManipulationEvaluation [[paper](https://arxiv.org/pdf/2506.06677)] [[project](https://robocerebra.github.io/)] 
- **RoboVerse**: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning [[paper](https://arxiv.org/pdf/2504.18904)] [[project](https://roboverseorg.github.io/)] ‚úÖ
- **AgiBot World Colosseo**: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems [[paper](https://arxiv.org/pdf/2503.06669)] [[project](https://agibot-world.com/)] ‚òÄÔ∏è
- **RoboTwin**: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2504.13059)] [[project](https://robotwin-benchmark.github.io/)] ‚úÖ
- **ManiSkill3**: Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI  [[paper](https://arxiv.org/pdf/2410.00425)] [[project](https://www.maniskill.ai/)] ‚úÖ
- **SimplerEnv**: Simulated Manipulation Policy Evaluation Environments for Real Robot Setups [[paper](https://arxiv.org/pdf/2405.05941)] [[project](https://simpler-env.github.io/)] ‚úÖ
- **LIBERO**: Benchmarking Knowledge Transfer for Lifelong Robot Learning [[paper](https://arxiv.org/pdf/2306.03310)] [[project](https://libero-project.github.io/main.html)] ‚úÖ
- **DISCOVERSE**: Efficient Robot Simulation in Complex High-Fidelity Environments [[paper](https://drive.google.com/file/d/1pG8N2qBdLuqj8_wylTYgsXYGOKMhwKXB/view)] [[project](https://air-discoverse.github.io/)] ‚úÖ

### üõ†Ô∏è Hardware Projects on Robotics 
- **Vision in Action**: Learning Active Perception from HumanDemonstrations [[paper](https://arxiv.org/pdf/2506.15666)] [[project](https://vision-in-action.github.io/)] ‚òÄÔ∏è
- **TWIST**: Teleoperated Whole-Body Imitation System [[paper](https://arxiv.org/pdf/2505.02833)] [[project](https://yanjieze.com/TWIST/)] üßê
- **Berkeley Humanoid Lite**: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot [[project](https://arxiv.org/pdf/2504.17249)] [[project](https://lite.berkeley-humanoid.org/)] ‚òÄÔ∏è
- **BEHAVIOR Robot Suite**: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities [[paper](https://arxiv.org/pdf/2503.05652)] [[project](https://behavior-robot-suite.github.io/)] ‚òÄÔ∏è ‚úÖ
- **Reactive Diffusion Policy**: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.02881)] [[project](https://reactive-diffusion-policy.github.io/)] ‚òÄÔ∏è
- **HOVER**: Versatile Neural Whole-Body Controller for Humanoid Robots [[paper](https://arxiv.org/pdf/2410.21229)] [[project](https://hover-versatile-humanoid.github.io/)] ‚òÄÔ∏è
- **Mobile ALOHA**: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation [[paper](https://arxiv.org/pdf/2401.02117)] [[project](https://mobile-aloha.github.io/)] ‚òÄÔ∏è 


### üî¨ Interdisciplinary
- The hippocampal sharp wave‚Äìripple in memory retrieval for immediate use and consolidation [[paper](https://www.nature.com/articles/s41583-018-0077-1)]  [[full text](https://pmc.ncbi.nlm.nih.gov/articles/PMC6794196/)]

---

## üôã Contributing

This repo is inspired by Yanjie Ze's [[Paperlist](https://github.com/YanjieZe/awesome-humanoid-robot-learning)]  
Feel free to submit pull requests for new papers, corrected links, or updated results.

---

## üìú License

[MIT](LICENSE)
