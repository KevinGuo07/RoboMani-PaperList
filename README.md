# A Paperlist for RoboMani-Learning üöÄü§ñ

[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)]()
<!--[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)-->

---

## üìå Basic Info

This repository collects the latest and influential papers and resources related to **robotic manipulation**. The focus is on:

- [Generalist Manipulation Models & Methods](#-generalist-manipulation-models-and-methods)
- [Reinforcement Learning (RL) on Robotics Manipulation](#-reinforcement-learning-rl-on-robotics-manipulation)
- [World Model in Robot Manipulation](#-world-model-in-robot-manipulation)
- [Skill Learning in Robotics](#-skill-learning-in-robotics) 
- [Data & Benchmarks](#-data-and-benchmarks) 
- [Hardware Projects on Robotics](#-hardware-projects-on-robotics) 
- [Interdisciplinary](#-interdisciplinary)

The following publications are ordered by time, with the most recent listed first.

Papers with **open-sourced implementations** are marked with a ‚òÄÔ∏è  
Papers with **real-world performance** reproduced by us are marked with a ‚úÖ

---

## üìö Paper List

### üß† Generalist Manipulation Models and Methods
- **Being-H0**: Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos [[paper](https://arxiv.org/pdf/2507.15597)] [[project](https://beingbeyond.github.io/Being-H0/)]
- **EgoVLA**: Learning Vision-Language-Action Models from Egocentric Human Videos [[paper](https://arxiv.org/pdf/2507.12440)] [[project](https://rchalyang.github.io/EgoVLA/)]
- **ThinkAct**: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning
- **GR-3** Technical Report [[paper](https://arxiv.org/pdf/2507.15493)] [[project](https://seed.bytedance.com/en/GR3)]
- **VITA**: VISION-TO-ACTION FLOW MATCHING POLICY [[paper](https://arxiv.org/pdf/2507.13231)] [[project](https://ucd-dare.github.io/VITA/)]
- **TACTILE-VLA**: UNLOCKING VISION-LANGUAGE ACTION MODEL‚ÄôS PHYSICAL KNOWLEDGE FOR TAC TILE GENERALIZATION [[paper](https://arxiv.org/pdf/2507.09160)] [[project]()]
- **VOTE**: Vision-Language-Action Optimization with Trajectory Ensemble Voting [[paper](https://arxiv.org/pdf/2507.05116)] [[project](https://github.com/LukeLIN-web/VOTE)]
- **Evo-0**: Vision-Language-Action Model with Implicit Spatial Understanding [[paper](https://www.arxiv.org/pdf/2507.00416)] [[project](https://mint-sjtu.github.io/Evo-VLA.io/)]
- **TRIVLA**: A TRIPLE-SYSTEM-BASED UNIFIED VISION-LANGUAGE-ACTION MODEL FOR GENERAL ROBOT CONTROL [[paper](https://arxiv.org/pdf/2507.01424)] [[project](https://zhenyangliu.github.io/TriVLA/)]
- **WorldVLA**: Towards Autoregressive Action World Model [[paper](https://arxiv.org/pdf/2506.21539)] [[project](https://github.com/alibaba-damo-academy/WorldVLA)] ‚òÄÔ∏è
- Grounding Language Models with Semantic Digital Twins for Robotic Planning [[paper](https://arxiv.org/pdf/2506.16493)] [[project]()]
- **Prompting with the Future**:Open-WorldModel PredictiveControlwithInteractiveDigitalTwins [[paper](https://arxiv.org/pdf/2506.13761)] [[project](https://prompting-with-the-future.github.io/)]
- **RationalVLA**: A Rational Vision-Language-Action Model with Dual System [[paper](https://arxiv.org/pdf/2506.10826)] [[project](https://irpn-eai.github.io/RationalVLA/)]
- **Chain-of-Action**: Trajectory Autoregressive Modeling for Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.09990)] [[project](https://chain-of-action.github.io/)]
- **BitVLA**: 1-bit Vision-Language-Action Models for Robotics Manipulation [[paper](https://arxiv.org/pdf/2506.07530)] [[project]()]
- **PDFactor**: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.pdf)]
- **FlowRAM**: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.pdf)]
- Real-Time Execution of Action Chunking Flow Policies [[paper](https://www.physicalintelligence.company/download/real_time_chunking.pdf)] [[project]()]
- **SmolVLA**: A Vision-Language-Action Model for Affordable and Efficient Robotics [[paper](https://arxiv.org/pdf/2506.01844)] [[project](https://huggingface.co/lerobot/smolvla_base)] ‚òÄÔ∏è
- Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better [[paper](https://arxiv.org/pdf/2505.23705)] [[project](https://www.physicalintelligence.company/research/knowledge_insulation)]
- **ChatVLA-2**: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge [[paper](https://arxiv.org/pdf/2505.21906)] [[project](https://chatvla-2.github.io/)]
- **ForceVLA**: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation [[paper](https://arxiv.org/pdf/2505.22159)] [[project](https://sites.google.com/view/forcevla2025/)]
- **DexUMI**: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation [[paper](https://arxiv.org/pdf/2505.21864)] [[project](https://dex-umi.github.io/)] ‚òÄÔ∏è
- **Hume**: Introducing System-2 Thinking in Visual-Language-Action Model [[paper](https://arxiv.org/pdf/2505.21432)] [[project]()]
- **FLARE**: Robot Learning with Implicit World Modeling [[paper](https://arxiv.org/pdf/2505.15659)] [[project]()]
- **InSpire**: Vision-Language-Action Models with Intrinsic Spatial Reasoning [[paper](https://arxiv.org/pdf/2505.13888)] [[project](https://koorye.github.io/proj/Inspire/)]
- **DreamGen**: Unlocking Generalization in Robot Learning through Neural Trajectories  [[paper](https://arxiv.org/pdf/2505.12705)] [[project](https://research.nvidia.com/labs/gear/dreamgen/)]
- **GLOVER++**: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation [[paper](https://arxiv.org/pdf/2505.11865)] [[project](https://teleema.github.io/projects/GLOVER++/)]
- **UniVLA**: Learning to Act Anywhere with Task-centric Latent Action [[paper](https://arxiv.org/pdf/2505.06111)] [[project](https://github.com/OpenDriveLab/UniVLA)] ‚òÄÔ∏è
- **NORA**: A SMALL OPEN-SOURCED GENERALIST VISION LANGUAGE ACTION MODEL FOR EMBODIED TASKS [[paper](https://arxiv.org/pdf/2504.19854)] [[project](https://declare-lab.github.io/nora)] ‚òÄÔ∏è
- **Gemini Robotics**: Bringing AI into the Physical World [[paper](https://arxiv.org/abs/2503.20020)] [[project](https://deepmind.google/models/gemini-robotics/)] ‚òÄÔ∏è
- **GR00T N1**: An Open Foundation Model for Generalist Humanoid Robots [[paper](https://arxiv.org/pdf/2503.14734)] [[project](https://developer.nvidia.com/isaac/gr00t)]
- **œÄ0.5**: a VLA with Open-World Generalization [[paper](https://www.physicalintelligence.company/download/pi05.pdf)] [[project](https://www.physicalintelligence.company/blog/pi05)] ‚òÄÔ∏è
- **PointVLA**: Injecting the 3D World into Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2503.07511)] [[project](https://pointvla.github.io/)]
- **GraspVLA**: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data [[paper](https://arxiv.org/pdf/2505.03233)] [[project](https://pku-epic.github.io/GraspVLA-web/)] ‚òÄÔ∏è
- **Reactive Diffusion Policy**: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.02881)] [[project](https://reactive-diffusion-policy.github.io/)] ‚òÄÔ∏è
- **Unified Video Action Model** [[paper](https://arxiv.org/pdf/2503.00200)] [[project](https://unified-video-action-model.github.io/)]
- **Helix**:Helix: A Vision-Language-Action Model for Generalist Humanoid Control [[project](https://www.figure.ai/news/helix)]
- **You Only Teach Once**: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations [[paper](https://arxiv.org/pdf/2501.14208)] [[project](https://hnuzhy.github.io/projects/YOTO/)]
- **TraceVLA**: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [[paper](https://arxiv.org/pdf/2412.10345)] [[project](https://tracevla.github.io/)] ‚òÄÔ∏è
- **CogACT**: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation [[paper](https://arxiv.org/pdf/2411.19650)] [[project](https://cogact.github.io/)] ‚òÄÔ∏è ‚úÖ
- **GRAPE**: Generalizing Robot Policy via Preference Alignment [[paper](https://arxiv.org/pdf/2411.19309)] [[project](https://grape-vla.github.io/)] ‚òÄÔ∏è
- **iDP3**: Generalizable Humanoid Manipulation with 3D Diffusion Policies [[paper](https://arxiv.org/pdf/2410.10803)] [[project](https://humanoid-manipulation.github.io/)] ‚òÄÔ∏è
- **œÄ0**: A Vision-Language-Action Flow Model for General Robot Control [[paper](https://arxiv.org/pdf/2410.24164)] [[project](https://www.physicalintelligence.company/blog/pi0)] ‚òÄÔ∏è ‚úÖ
- **TinyVLA**: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.12514)] [[project](https://arxiv.org/pdf/2409.12514)] ‚òÄÔ∏è
- **ReKep**: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation [[paper](https://arxiv.org/pdf/2409.01652)] [[project](https://rekep-robot.github.io/)] ‚òÄÔ∏è ‚úÖ
- **OpenVLA**: An Open-Source Vision-Language-Action Model [[paper](https://arxiv.org/pdf/2406.09246)] [[project](https://openvla.github.io/)] ‚òÄÔ∏è ‚úÖ
- **3D Diffusion Policy**: Generalizable Visuomotor Policy Learning via Simple 3D Representations [[paper](https://arxiv.org/pdf/2403.03954)] [[project](https://3d-diffusion-policy.github.io/)] ‚òÄÔ∏è ‚úÖ

### üîÅ Reinforcement Learning (RL) on Robotics Manipulation
- Reinforcement Learning for Flow-Matching Policies [[paper](https://arxiv.org/pdf/2507.15073v1)]
- **FOUNDER**: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making [[paper](https://arxiv.org/pdf/2507.12496)] [[project](https://sites.google.com/view/founder-rl)]
- **EXPO**: Stable Reinforcement Learning with Expressive Policies [[paper](https://arxiv.org/pdf/2507.07986)] 
- Asynchronous multi-agent deep reinforcement learning under partial observability [[paper](https://journals.sagepub.com/doi/full/10.1177/02783649241306124)]
- Reinforcement Learning with Action Chunking [[paper](https://www.alphaxiv.org/abs/2507.07969v1)] [[project](https://github.com/ColinQiyangLi/qc)]
- **SimLauncher**: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training [[paper](https://arxiv.org/pdf/2507.04452)] [[project]()]
- **RLRC**: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2506.17639)] [[project]()]
- Steering Your Diffusion Policy with Latent Space Reinforcement Learning [[paper](https://arxiv.org/pdf/2506.15799)] [[project](https://diffusion-steering.github.io/)]
- **GMT**:General Motion Tracking for Humanoid Whole-Body Control [[paper](https://arxiv.org/pdf/2506.14770)] [[project](https://gmt-humanoid.github.io/)]
- **Eye, Robot**: Learning to Look to Act with a BC-RL Perception-Action Loop [[paper](https://arxiv.org/pdf/2506.10968)] [[project](https://www.eyerobot.net/)]
- Reinforcement Learning via Implicit Imitation Guidance [[paper](https://arxiv.org/pdf/2506.07505)] [[project]()]
- Robotic Policy Learning via Human-assisted Action Preference Optimization [[paper](https://arxiv.org/pdf/2506.07127)] [[project](https://gewu-lab.github.io/hapo_human_assisted_preference_optimization/)]
- Self-Adapting Improvement Loops for Robotic Learning [[paper](https://arxiv.org/pdf/2506.07505)] [[project](https://diffusion-supervision.github.io/sail/)]
- **Robot-R1**: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics [[paper](https://arxiv.org/pdf/2506.00070v1)] [[project]()] 
- Self-Challenging Language Model Agents [[paper](https://arxiv.org/pdf/2506.01716)] [[project]()] 
- Diffusion Guidance Is a Controllable Policy Improvement Operator [[paper](https://arxiv.org/pdf/2505.23458)] [[project](https://github.com/kvfrans/cfgrl)]
- **Beyond Markovian**: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [[paper](https://arxiv.org/pdf/2505.20561v1)] [[project](https://github.com/shenao-zhang/BARL)]
- What Can RL Bring to VLA Generalization? An Empirical Study [[paper](https://arxiv.org/pdf/2505.19789)] [[project](https://rlvla.github.io/)] 
- Learning to Reason without External Rewards [[paper](https://arxiv.org/pdf/2505.19590)] [[project]()]
- **GenPO**: Generative Diffusion Models Meet On-Policy Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.18763)] [[project](https://arxiv.org/html/2505.18763v1)]
- **VLA-RL**: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.18719)][[code](https://github.com/GuanxingLu/vlarl)] ‚òÄÔ∏è ‚úÖ
- **SimpleVLA-RL**: Online RL with Simple Reward Enables Training VLA Models with Only One Trajectory [[paper]()][[code](https://github.com/PRIME-RL/SimpleVLA-RL)] ‚òÄÔ∏è
- **TeViR**: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.19769)] [[project]()] 
- **Genie Centurion**: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance [[paper](https://arxiv.org/pdf/2505.18793)] [[project](https://genie-centurion.github.io/)] 
- **RIPT-VLA**: Interactive Post-Training for Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2505.17016)] [[project](https://ariostgx.github.io/ript_vla/)] ‚òÄÔ∏è 
- **ManipLVM-R1**: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models [[paper](https://arxiv.org/pdf/2505.16517)] [[project]()]
- Deep reinforcement learning for robotic manipulation [[technique report](https://patentimages.storage.googleapis.com/7f/04/95/2437c0dc1b5ab6/US20250153352A1.pdf)]
- **DORA**:Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation [[paper](https://arxiv.org/pdf/2505.14819)] [[project](https://sites.google.com/view/dora-manip)]
- What Matters for Batch Online Reinforcement Learning in Robotics? [[paper](https://arxiv.org/pdf/2505.08078)] [[project]()] 
- **ReinboT**: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning [[paper](https://arxiv.org/pdf/2505.07395)] [[project]()] 
- **IN‚ÄìRIL**: Interleaved Reinforement and Imitation Learning for Policy Fine-tuning [[paper](https://arxiv.org/pdf/2505.10442)] [[project](https://github.com/ucd-dare/IN-RIL)]
- **MoRE**: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models[[paper](https://arxiv.org/pdf/2503.08007)] [[project]()]
- **ConRFT**: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[paper](https://arxiv.org/pdf/2502.05450)][[project](https://cccedric.github.io/conrft/)] ‚òÄÔ∏è
- Flow Q-Learning [[paper](https://arxiv.org/pdf/2502.02538)][[project](https://seohong.me/projects/fql/)]
- Improving Vision-Language-Action Model with Online Reinforcement Learning [[paper](https://arxiv.org/pdf/2501.16664)] [[project]()] 
- **RLDG**: Robotic Generalist Policy Distillation via Reinforcement Learning [[paper](https://arxiv.org/pdf/2412.09858)] [[project](https://generalist-distillation.github.io/)] ‚òÄÔ∏è 
- **Hil-serl**: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning [[paper](https://arxiv.org/pdf/2410.21845)] [[project](https://hil-serl.github.io/)] ‚òÄÔ∏è ‚úÖ
- **SERL**: A Software Suite for Sample-Efficient Robotic Reinforcement Learning [[paper](https://arxiv.org/pdf/2401.16013)] [[project](https://serl-robot.github.io/)] ‚òÄÔ∏è

### üåç World Model in Robot Manipulation 
- Latent Policy Steering with Embodiment-Agnostic Pretrained World Models [[paper](https://arxiv.org/pdf/2507.13340)] [[project]()]
- **FOUNDER**: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making [[paper](https://arxiv.org/pdf/2507.12496)] [[project](https://sites.google.com/view/founder-rl)]
- **EmbodieDreamer**: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling [[paper](https://arxiv.org/pdf/2507.05198)] [[project](https://embodiedreamer.github.io/)] 
- **DreamVLA**: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge [[paper](https://arxiv.org/pdf/2507.04447)] [[project](https://zhangwenyao1.github.io/DreamVLA/)] 
- A Survey: Learning Embodied Intelligence from Physical Simulators and World Models [[paper](https://arxiv.org/pdf/2507.00917)] [[project](https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey)] 
- **RoboScape**: Physics-informed Embodied World Model [[paper](https://arxiv.org/pdf/2506.23135)] [[project](https://github.com/tsinghua-fib-lab/RoboScape)] 
- **ParticleFormer**: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.23126)] [[project](https://particleformer.github.io/)] 
- **World4Omni**: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation [[paper](https://arxiv.org/pdf/2506.23919)] [[project](https://world4omni.github.io/)]
- **RoboPearls**: Editable Video Simulation for Robot Manipulation [[paper](https://arxiv.org/pdf/2506.22756)] [[project](https://tangtaogo.github.io/RoboPearls/)] 
- Prompting with the Future: Open-WorldModel PredictiveControlwithInteractiveDigitalTwins [[paper](https://arxiv.org/pdf/2506.13761)] [[project](https://prompting-with-the-future.github.io/)]
- **FLARE**: Robot Learning with Implicit World Modeling [[paper](https://arxiv.org/pdf/2505.15659)] [[project](https://research.nvidia.com/labs/gear/flare/)] 
- Occupancy World Model for Robots [[paper](https://arxiv.org/pdf/2505.05512)]
- Learning 3D Persistent Embodied World Models [[paper](https://arxiv.org/pdf/2505.05495)]
- **iVideoGPT**: Interactive VideoGPTs are Scalable World Models [[paper](https://arxiv.org/pdf/2405.15223)] [[project](https://thuml.github.io/iVideoGPT/)] ‚òÄÔ∏è ‚úÖ

### ü¶æ Skill Learning in Robotics

### üì¶ Data and Benchmarks
- **FreeTacMan**: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation [[paper](https://arxiv.org/pdf/2506.01941)] [[project](https://freetacmanblog.github.io/)] 
- **Guiding Data Collection**: via Factored Scaling Curves [[paper](https://arxiv.org/pdf/2505.07728)] [[project]()] 
- **DemoGen**: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning [[paper](https://arxiv.org/pdf/2502.16932)] [[project](https://demo-generation.github.io/)] ‚òÄÔ∏è
- Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition  [[paper](https://arxiv.org/pdf/2407.00299)] [[project](https://norweig1an.github.io/HAJL.github.io/)] 
- **AutoBio**: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory [[paper](https://arxiv.org/pdf/2505.14030) [[project](https://github.com/autobio-bench/AutoBio)]
- **3DFlowAction**: Learning Cross-Embodiment Manipulation from 3D Flow World Model [[paper](https://arxiv.org/pdf/2506.06199) [[project](https://github.com/Hoyyyaard/3DFlowAction/)]
- A very good survey and report on Simulators [[project](https://simulately.wiki)]
- **EgoDex**: Learning Dexterous Manipulation from Large-Scale Egocentric Video [[paper](https://arxiv.org/pdf/2505.11709)] [[project]()]
- **Open X-Embodiment**: Robotic Learning Datasets and RT-X Model [[paper](https://arxiv.org/pdf/2310.08864)] [[project](https://robotics-transformer-x.github.io/)] ‚úÖ
- **RoboArena**: Distributed Real-World Evaluation of Generalist Robot Policies [[paper](https://robo-arena.github.io/assets/roboarena-B1XSLVwD.pdf)] [[project](https://robo-arena.github.io/)]
- **RoboCerebra**:ALarge-scaleBenchmarkfor Long-horizonRoboticManipulationEvaluation [[paper](https://arxiv.org/pdf/2506.06677)] [[project](https://robocerebra.github.io/)] 
- **RoboVerse**: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning [[paper](https://arxiv.org/pdf/2504.18904)] [[project](https://roboverseorg.github.io/)] ‚úÖ
- **AgiBot World Colosseo**: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems [[paper](https://arxiv.org/pdf/2503.06669)] [[project](https://agibot-world.com/)] ‚òÄÔ∏è
- **RoboTwin**: Dual-Arm Robot Benchmark with Generative Digital Twins [[paper](https://arxiv.org/pdf/2504.13059)] [[project](https://robotwin-benchmark.github.io/)] ‚úÖ
- **ManiSkill3**: Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI  [[paper](https://arxiv.org/pdf/2410.00425)] [[project](https://www.maniskill.ai/)] ‚úÖ
- **SimplerEnv**: Simulated Manipulation Policy Evaluation Environments for Real Robot Setups [[paper](https://arxiv.org/pdf/2405.05941)] [[project](https://simpler-env.github.io/)] ‚úÖ
- **LIBERO**: Benchmarking Knowledge Transfer for Lifelong Robot Learning [[paper](https://arxiv.org/pdf/2306.03310)] [[project](https://libero-project.github.io/main.html)] ‚úÖ
- **DISCOVERSE**: Efficient Robot Simulation in Complex High-Fidelity Environments [[paper](https://drive.google.com/file/d/1pG8N2qBdLuqj8_wylTYgsXYGOKMhwKXB/view)] [[project](https://air-discoverse.github.io/)] ‚úÖ

### üõ†Ô∏è Hardware Projects on Robotics 
- **Vision in Action**: Learning Active Perception from HumanDemonstrations [[paper](https://arxiv.org/pdf/2506.15666)] [[project](https://vision-in-action.github.io/)]
- **TWIST**: Teleoperated Whole-Body Imitation System [[paper](https://arxiv.org/pdf/2505.02833)] [[project](https://yanjieze.com/TWIST/)]
- **Berkeley Humanoid Lite**: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot [[project](https://arxiv.org/pdf/2504.17249)] [[project](https://lite.berkeley-humanoid.org/)]
- **BEHAVIOR Robot Suite**: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities [[paper](https://arxiv.org/pdf/2503.05652)] [[project](https://behavior-robot-suite.github.io/)] ‚òÄÔ∏è ‚úÖ
- **Reactive Diffusion Policy**: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [[paper](https://arxiv.org/pdf/2503.02881)] [[project](https://reactive-diffusion-policy.github.io/)]
- **HOVER**: Versatile Neural Whole-Body Controller for Humanoid Robots [[paper](https://arxiv.org/pdf/2410.21229)] [[project](https://hover-versatile-humanoid.github.io/)] ‚òÄÔ∏è
- **Mobile ALOHA**: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation [[paper](https://arxiv.org/pdf/2401.02117)] [[project](https://mobile-aloha.github.io/)] ‚òÄÔ∏è 


### üî¨ Interdisciplinary
- The hippocampal sharp wave‚Äìripple in memory retrieval for immediate use and consolidation [[paper](https://www.nature.com/articles/s41583-018-0077-1)]  [[full text](https://pmc.ncbi.nlm.nih.gov/articles/PMC6794196/)]

---

## üôã Contributing

This repo is inspired by Yanjie Ze's [[Paperlist](https://github.com/YanjieZe/awesome-humanoid-robot-learning)]  
Feel free to submit pull requests for new papers, corrected links, or updated results.

---

## üìú License

[MIT](LICENSE)
